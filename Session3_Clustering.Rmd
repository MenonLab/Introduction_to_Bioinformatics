---
title: "Bioinformatics Mini-Course Session 3 - Clustering"
author: "Vilas Menon"
output: html_notebook
---

##Load the data file 
###This data file contains the percentage of foreign-born people from each country living in each state in the US
```{r}
data1 <- read.csv("place_of_birth_table_refined.csv",as.is=T,row.names=1,header=T)
###switch rows and columns of the matrix
data1 <- t(data1)
data1[1:5,1:5]
```

##Hierarchical clustering
###Specify a distance metric and a grouping method - here, use Euclidean distance and complete linkage
```{r}
#By default, the hcluster function clusters the rows of a data.frame
distance_values <- dist(data1,method = "euclidean")
clustering <- hclust(distance_values,method = "complete")
plot(clustering,hang=-1)
```
###Try a different distance metric (1 - Pearson correlation)
```{r}
###Possible choices 
distance_values <- as.dist(1-cor(t(data1)))
clustering <- hclust(distance_values,method = "complete")
plot(clustering,hang=-1)
```
###Try playing around with various grouping methods
```{r}
grouping_method = "single"   ###possible choices: "single", "average", "complete", "ward.D"
distance_values <- as.dist(1-cor(t(data1)))
clustering <- hclust(distance_values,method = grouping_method)
plot(clustering,hang=-1)
```
##Cutting the tree to generate a specific number of clusters
```{r}
###Generate hierarchical tree
distance_values <- as.dist(1-cor(t(data1)))
clustering <- hclust(distance_values,method = "complete")
plot(clustering,hang=-1)

###Cut the tree to get 7 clusters
clusters <- cutree(clustering,k = 7)
hclust_correlation_complete_7=data.frame(cluster=clusters[clustering$order])
hclust_correlation_complete_7
```
##Cutting the tree at a specific height (distance metric)
```{r}
###Generate hierarchical tree
distance_values <- as.dist(1-cor(t(data1)))
clustering <- hclust(distance_values,method = "complete")
plot(clustering,hang=-1)

###Cut the tree at a specific height
abline(h=0.6,lty=2)
clusters <- cutree(clustering, h= 0.6)
hclust_correlation_complete_0.6=data.frame(cluster=clusters[clustering$order])
hclust_correlation_complete_0.6
```

##K-means clustering
###The default distance metric is the Euclidean distance, whereas the algorithm can be selected
###Note that k-means clustering has randomness, since the initial cluster centers are selected randomly - to ensure reproducibility, need to set a random seed
```{r}
set.seed(0) ###setting the random seed
###k-means clustering, specifying 7 clusters
clustering_kmeans=kmeans(data1,center=7)
kmeans_7=data.frame(cluster=clustering_kmeans$cluster[order(clustering_kmeans$cluster)])
kmeans_7
```

##Visualizing clusters in reduced dimension space
###Principal component analysis as a reduced dimension representation
```{r}
#PCA on raw data
pca_output=prcomp(data1)
#plot states along PC1 and PC2
plot(pca_output$x[,c(1,2)],type='n',main="PCA, unnormalized")
text(pca_output$x[,c(1,2)],rownames(pca_output$x))

#z-score the data and then run PCA
data1_zscored = t(scale(t(data1)))
pca_output_zscored=prcomp(data1_zscored)
#plot states along PC1 and PC2
plot(pca_output_zscored$x[,c(1,2)],type='n',main="PCA, after z-scoring")
text(pca_output_zscored$x[,c(1,2)],rownames(pca_output_zscored$x))


```
##How well do the hierarchical clusters line up with reduced dimension visualization?
```{r}
##select clusters colors, depending on the clustering
cluster_colors=rainbow(max(hclust_correlation_complete_7))[hclust_correlation_complete_7[,1]]
names(cluster_colors)=rownames(hclust_correlation_complete_7)

#PCA on raw data
pca_output=prcomp(data1)
#z-score the data and then run PCA
data1_zscored = t(scale(t(data1)))
pca_output_zscored=prcomp(data1_zscored)
#plot states along PC1 and PC2
plot(pca_output_zscored$x[,c(1,2)],type='n',main="PCA, after z-scoring")
text(pca_output_zscored$x[,c(1,2)],rownames(pca_output_zscored$x),col=cluster_colors[rownames(pca_output_zscored$x)])
```

##How well do the kmeans clusters line up with reduced dimension visualization?
```{r}
##select clusters colors, depending on the clustering
cluster_colors=rainbow(max(kmeans_7))[kmeans_7[,1]]
names(cluster_colors)=rownames(kmeans_7)

#PCA on raw data
pca_output=prcomp(data1)
#z-score the data and then run PCA
data1_zscored = t(scale(t(data1)))
pca_output_zscored=prcomp(data1_zscored)
#plot states along PC1 and PC2
plot(pca_output_zscored$x[,c(1,2)],type='n',main="PCA, after z-scoring")
text(pca_output_zscored$x[,c(1,2)],rownames(pca_output_zscored$x),col=cluster_colors[rownames(pca_output_zscored$x)])
```


##Exercise 1:
####Select two different distance metrics (Euclidean or 1-correlation) and two different hierarchical grouping methods (single, average, complete, ward.D), cut the tree to generate 7 clusters in each, and plot the clusters on the PC plot




##Exercise 2:
####Select multiple numbers of clusters for k-means clustering and plot them on the PCA plot






##Advanced analysis: which features distinguish specific clusters from each other?
```{r}
##Select clustering###
clustering_scheme=hclust_correlation_complete_7[rownames(data1),1]

##Select cluster - the "east coast" states
cluster_id1=3 

###find differential features (columns of data1) using a t-test
differential_pvalues=apply(data1,2,function(x){t.test(x[which(clustering_scheme==cluster_id1)],x[which(clustering_scheme!=cluster_id1)])$p.value})
mean_ratio=apply(data1,2,function(x){mean(x[which(clustering_scheme==cluster_id1)])/mean(x[which(clustering_scheme!=cluster_id1)])})
differential_pvalues=p.adjust(differential_pvalues,"BH")

###identify which features have p<0.05
differential_features=which(differential_pvalues<0.05 & mean_ratio>1)

###plot differential features
for (ii in differential_features) {
  boxplot(data1[,ii]~clustering_scheme,main=colnames(data1)[ii],ylab="Percenage",xlab="Cluster")
}


```


